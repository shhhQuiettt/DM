{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining laboratory - Introduction\n",
    "\n",
    "Welcome to the data mining class. During our meetings, we will be dealing with processing and exploring data with the use of the Python language in the Jupyter Notebook setting. We are also going to use low-code and no-code solutions to the presented problems. Today, we are going to set up our working stations and get familiar with the setup.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Course assignements\n",
    "\n",
    "This course consists of X notebooks, X homeworks and 3 assignments. In order to get a pass mark, you need to complete all homeworks. You can get a maximum of 4 points for each assignment. Once the assignment is announced you have two weeks to complete it. Each week of delay deducts 1 point from the mark you get. The amount of points you gather during the course will indicate your final grade.\n",
    "\n",
    "| Points| Grade |\n",
    "| --- | --- |\n",
    "| 0| 2.0  |\n",
    "| 6 | 3.0 |\n",
    "| 7.5 | 3.5 |\n",
    "| 9 | 4.0 |\n",
    "| 10.5 | 4.5 |\n",
    "| 11.5 | 5.0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is the Jupyter Notebook?\n",
    "\n",
    "It's a computing platform that is very commonly used for code presentation, on-hand code execution, as well as preparing code snippets, which later on might be used in a larger library. In this setting you can easily combine Markdown text and executable Python code. This format is very popular in machine learning, data mining and artificial intelligence field in general. A single file in this setting is very often referred to as just a *Notebook*. The file you are viewing right now is a Notebook. Notebook files are usually named with the extension *.ipynb*, which stems from the original open-source project name *IPython Notebook*. The Notebook uses an interactive kernel,  which allows us to maintain the current execution of the code. During the execution, all variables, defined functions, and classes, etc. are stored in the memory, which gives us flexible access to everything we coded (this is nothing new compared to a standard Python interpreter). The Notebooks are delivered to us in several different settings, here are some:\n",
    "  \n",
    "  - **Advanced, modern IDE, which supports Jupyter Notebooks.** In this setting, the IDE is responsible for setting up the interactive kernel with the use of the Python interpreter. A good example of an IDE, which supports the Jupyter Notebooks is Visual Studio Code. Prior to using this option, we need to set up the Python interpreter on the machine.\n",
    "  \n",
    "      **Pros**:\n",
    "\n",
    "         * full customization\n",
    "         * full access to data on hand\n",
    "         * usually supports version control\n",
    "         * easy setup process\n",
    "      **Cons**:\n",
    "      \n",
    "         * you need to set up an IDE on every machine you work on\n",
    "         * requires installation of Python interpreter on the machine\n",
    "\n",
    "\n",
    "  - **A stand-alone Jupyter Notebook server.** This is the original method of delivering the Notebooks. In order to use this setting, one must download and run the Jupyter Server as a separate process on a machine on hand. The Jupyter Notebook server often comes in bundle with complete Python distributions (e.g. WinPython), in that case, the server executable file is usually within the Python folder. The Jupyter Notebook server allows us to access, view and run the notebooks via the web application accessible through a browser. The server allows us to set up the connection details (e.g. the IP address, port, authentication method, password). If you want to use the server in a public network. you need to be very careful while using this option, as it allows an easy access to the Remote Code Execution, which is a substantial vulnerability. Whoever has the access to the *Notebooks* via the server, essentially has the same privileges, as the user, who started the server. Nothing stops us from using the server on the *localhost*. Running the server in a default setting is as simple as running the command:\n",
    "\n",
    "                 jupyter notebook\n",
    "  \n",
    "      Once the server is running, you have access to files and directories, starting with the directory on which, the server was started. Opening the notebook file, switches the application view, so that you can execute the code and read the markdown.\n",
    "      \n",
    "      **Pros**:\n",
    "\n",
    "         * full customization\n",
    "         * access to data on the server machine\n",
    "         * ability to use it in a network setting with many users and a single server\n",
    "      **Cons**:\n",
    "      \n",
    "         * fairly hard setup process (if you want to use it with several users in a network setting)\n",
    "         * if you do not have a server machine, you can only run it in an offline setting\n",
    "         * no native support for version control\n",
    "         * requires installation of Python interpreter on the machine\n",
    "          \n",
    "\n",
    "  - **External Notebook server paired up with virtual machine.** In this setting, we are using a virtual machine with a temporary python environment as the working space. Although we are not forced to maintain the Notebook server, this option comes with several limitations. We are forced to follow the rules of the virtual machine provider. Usually we not permitted to use such a notebook in order to host data, download torrents, use it as an SSH server, connect to the remote proxy, etc. (nothing really related to Data Mining). Such a notebook does not have direct access to our files, we usually need to upload the data on the virtual machine (or a cloud drive) in order to process the data. Other than that, we can consume the Nootebook files as normal. A good example of this setting is Google Colaboratory.\n",
    "\n",
    "      **Pros**:\n",
    "\n",
    "         * access to notebooks on any machine with no setup\n",
    "         * limited customization\n",
    "         * ability to modify and create new Notebooks on hand on any machine\n",
    "         * no need to install any software on the machine (except for a browser)\n",
    "      **Cons**:\n",
    "      \n",
    "         * no support for version control\n",
    "         * restrictions of use\n",
    "         * requires an account (e.g. Google Account)\n",
    "         * limited access to data on hand\n",
    "         * requires uploading the data to an external server (usually limitted space)\n",
    "         * limited customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course I propose one of the two options - those options are not obligatory, you can use any setup you want:\n",
    " - Visual Studio Code\n",
    " - Google Colaboratory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Visual Studio Code.\n",
    "\n",
    "In the class we will be using the Google Colaboratory service. However, if you want to make your setup at home, or with a personal laptop, you can use the Visual Studio Code setup. Process of setting it up comprised of 2 (pretty obvious) steps:\n",
    " - installing Python interpreter - \n",
    "    - if you are using a Linux machine, it is very likely you already have the Python interpreter installed. If this is not the case, use your default package manager to install python (i.e. `apt install python3` on Debianoids).\n",
    "    - if you are using a Windows machine I suggest using a [WinPython](https://winpython.github.io/) package. It comes with a pre-installed set of libraries.\n",
    "    - you can also use the [default Python installer](https://www.python.org/downloads/).\n",
    " - installing Visual Studio Code - VSC is an multi-platform IDE. You can find it [here](https://code.visualstudio.com/).\n",
    " \n",
    "Once you have everything installed you need to create a space on the computer for this class (we are going to use toy data sets, so you do not need gigabytes of free space). You start by creating a dedicated directory on your hard drive. Download this notebook (.ipynb version, not the html) and paste it into the newly created directory. Then, you open the Visual Studio Code application and from the File menu you choose the Open Directory option. In the file explorer you should be able to see this notebook. Upon the first execution of the code block you will need to choose a Python interpreter, which you have already installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Google Colaboratory\n",
    "\n",
    "Using Google Colab is much easier. You just need to download this notebook, log in to your Google account on the [Colaboratory website](https://colab.research.google.com/). From the File menu use the \"Send notebook\" option. Choose the downloaded file. That's it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have everything set up, switch from the HTML version of the notebook to the interactive one (either in Colab or in VSC). Starting the next week you will be downloading and opening the notebook at the beginning of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is a Notebook organized?\n",
    "\n",
    "Each Notebook consists of list of cells. There are two types of cells:\n",
    "\n",
    " - **Code cell** - the code cell is filled with the code in the programming language the Notebook is set up for (usually it's Python). You can execute the code and immediately see the result. Everything that *happened* in the execution is available in the next cell you run. Once the code cell is executed, it is annotated with a number, which refers to the order of execution. The first cell you run will be annotated with number [1], second with number [2], etc. The enumeration helps us to keep up with the current status of execution.\n",
    " - **Markdown cell** - the markdown cell allows us to insert a formatted text into the notebook. The text is formatted with use of the [Markdown](https://www.markdownguide.org/) language. The Markdown is a lightweight markup language, which is used to add simple formatting to plaintext documents. It was created in 2004 by John Gruber. It is one of the most popular markup languages. This is the same language you can use for example in the Discord app.\n",
    " \n",
    "\n",
    " Each of the code cells can be executed at any point. In most of the IDEs we are allowed to run all cells at once, restart the interpreter and clear all variables and definitions, add a new cell, and reorder existing the cells. \n",
    "\n",
    "\n",
    " #### Exercise 1. \n",
    " \n",
    " Execute the cells in the following order:\n",
    "   1. Run cell 2\n",
    "   2. Run cell 1\n",
    "   3. Run cell 3\n",
    "   3. Run cell 2\n",
    "   4. Run cell 3\n",
    "   5. Restart the kernel\n",
    "   6. Run cell 1\n",
    "   7. Run cell 2\n",
    "   8. Run cell 3.\n",
    "   9. Run cell 3.\n",
    "\n",
    "Observe the results and make notes. Can we execute the cell 2 immediately, why? How does the annotation change when we run a single cell multiple times? What is the value of the _ expression? You can restart this exercise by restarting the kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a + 2\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a + _\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.\n",
    "\n",
    "Create a new markdown cell directly below this one and use the Markdown language to answer the questions asked in Exercise 1. Use the following features:\n",
    "  - Level 4 heading\n",
    "  - Bullet list\n",
    "  - Bold text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell magic\n",
    "\n",
    "In order to use a package in your Python script you need to import it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens when the package is not installed on the machine? Well. Probably you need open the terminal, type an apropriate command and download the package. This even is more complicated when you have no direct access to the machine. In this case we can use something called [cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html). Ususally the *Code cell* is interpreted as a python script. However, we can add a special decorator to change its behaviour. When we add `%%bash` at the beginning of the cell it is going to be executed as if it was a bash terminal. So, in order to install the numpy package (it should be already installed), you can create a cell similar to this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/krzys/miniconda3/lib/python3.11/site-packages (1.26.0)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%%bash` is not the only magical command out there. Sometimes we will compare time of executions of different code variants. In this case we can use the `%%time` or `%%timeit` magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 ms, sys: 263 µs, total: 1.92 ms\n",
      "Wall time: 1.25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = np.zeros((10000,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.4 s, sys: 303 ms, total: 2.7 s\n",
      "Wall time: 2.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = [[0 for _ in range(10000)] for _ in range(10000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514 µs ± 42.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = np.zeros((1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.7 ms ± 595 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = [[0 for _ in range(1000)] for _ in range(1000)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full list of cell magics can be found [here](https://ipython.readthedocs.io/en/stable/interactive/magics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy data sets\n",
    "\n",
    "During the course we will be using different data sets in order to get familiar with data mining techiques. This section illustrates several techniques of loading up the data sets.\n",
    "\n",
    "#### Scikit-learn package\n",
    "\n",
    "Among various packages we are going to use the scikit-learn package (sklearn). Today we will get familiar with the toy data sets, which the package provides. The package provides 7 different data sets (including boston data set, which is deprecated), among them:\n",
    "\n",
    "- Iris data set - The famous Iris database, first used by Sir R.A. Fisher.\n",
    "- Digits data set - The data set contains images of hand-written digits: 10 classes where each class refers to a digit.\n",
    "- Wine data set - The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators.\n",
    "\n",
    "#### Loading the data set\n",
    "\n",
    "The datasets are loaded into a dictionary-like structure, [sklearn.utils.Bunch](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html). We use a set of dedicated *load* functions to load the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_breast_cancer, load_digits, load_diabetes, load_linnerud, load_wine\n",
    "\n",
    "iris_data_set = load_iris()\n",
    "breast_cancer_data_set = load_breast_cancer()\n",
    "digits_data_set = load_digits()\n",
    "diabetes_data_set = load_diabetes()\n",
    "linnerud_data_set = load_linnerud()\n",
    "wine_data_set = load_wine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain a description of each of the data sets by using the DESCR field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "|details-start|\n",
      "**References**\n",
      "|details-split|\n",
      "\n",
      "- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "  Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "  Structure and Classification Rule for Recognition in Partially Exposed\n",
      "  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "  on Information Theory, May 1972, 431-433.\n",
      "- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "  conceptual clustering system finds 3 classes in the data.\n",
      "- Many, many more ...\n",
      "\n",
      "|details-end|\n"
     ]
    }
   ],
   "source": [
    "print(iris_data_set.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data set consists of a list of entries. Each entry is comprised of a set of features. Each feature has a name, which corresponds to its real source. We can obtain the names of features by using the feature_names field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data_set.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in each of the data sets is organized as a numpy array (more on that next week). We can get to it by using the data field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(iris_data_set.data))\n",
    "iris_data_set.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry corresponds to a certain class, we can obtain names of the classes with use of the target_names field, and the list of classes corresponding to each entry with the target field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(iris_data_set.target_names)\n",
    "print(iris_data_set.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 1.\n",
    "\n",
    "Write a function, which processes a sci-kit learn Bunch object. The function is expected to prepare a the data set description. The description has the following format:\n",
    "\n",
    "  `Dataset data_set_name.`\n",
    "\n",
    "  `Number of samples: NNN`\n",
    "\n",
    "  `Number of classes: NNN`\n",
    "\n",
    "  `  Number of samples in class target_name1: NNN`\n",
    "\n",
    "  `  Number of samples in class target_name2: NNN`\n",
    "\n",
    "  `  ...`\n",
    "\n",
    "  `Number of features: NNN`\n",
    "\n",
    "  `  Average value of feature feature_name1: NNN`\n",
    "\n",
    "  `  Standard deviation of feature feature_name1: NNN`\n",
    "\n",
    "  `  Average value of feature feature_name2: NNN`\n",
    "\n",
    "  `  Standard deviation of feature feature_name2: NNN`\n",
    "\n",
    "  `  Average value of feature feature_name3: NNN`\n",
    "\n",
    "  `  Standard deviation of feature feature_name3: NNN`\n",
    "  \n",
    "  `  ...`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_bunch.py:54\u001b[0m, in \u001b[0;36mBunch.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_bunch.py:39\u001b[0m, in \u001b[0;36mBunch.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     35\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecated_key_to_warnings[key],\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m     )\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'name'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     description \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data_set\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m description\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(prepare_dataset_description(iris_data_set, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIris\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(prepare_dataset_description(breast_cancer_data_set, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBC\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(prepare_dataset_description(digits_data_set, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDigits\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m, in \u001b[0;36mprepare_dataset_description\u001b[0;34m(data_set, data_set_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_dataset_description\u001b[39m(data_set : Bunch, data_set_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      4\u001b[0m     description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m     description \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data_set\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m description\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_bunch.py:56\u001b[0m, in \u001b[0;36mBunch.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(key)\n",
      "\u001b[0;31mAttributeError\u001b[0m: name"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import Bunch\n",
    "import numpy as np\n",
    "def prepare_dataset_description(data_set : Bunch, data_set_name) -> str:\n",
    "    description = \"\"\n",
    "    description += data_set.name\n",
    "    return description\n",
    "\n",
    "print(prepare_dataset_description(iris_data_set, 'Iris'))\n",
    "print(prepare_dataset_description(breast_cancer_data_set, 'BC'))\n",
    "print(prepare_dataset_description(digits_data_set, 'Digits'))\n",
    "print(prepare_dataset_description(diabetes_data_set, 'Diabetes'))\n",
    "print(prepare_dataset_description(linnerud_data_set, 'Linnerud'))\n",
    "print(prepare_dataset_description(wine_data_set, 'Wine'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0cc2e28e15ec3d399ff2fa987eff54814158b78b1ea93d5ce0744d3e4b658fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
